{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange\n",
    "from typing import Union, Tuple\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_conv1d_minimal` passed!\n"
     ]
    }
   ],
   "source": [
    "def conv_transpose1d_minimal(x: t.Tensor, weights: t.Tensor) -> t.Tensor:\n",
    "    '''Like torch's conv_transpose1d using bias=False and all other keyword arguments left\n",
    "    at their default values.\n",
    "\n",
    "    x: shape (batch, in_channels, width)\n",
    "    weights: shape (in_channels, out_channels, kernel_width)\n",
    "\n",
    "    Returns: shape (batch, out_channels, output_width)\n",
    "    '''\n",
    "    kernel_width = weights.shape[-1]\n",
    "    pad_size = kernel_width - 1\n",
    "\n",
    "    x = t.nn.functional.pad(x, (pad_size, pad_size))\n",
    "    weights = rearrange(weights, \"i o w -> o i w\")\n",
    "    weights = t.flip(weights, (-1,))\n",
    "\n",
    "    return t.nn.functional.conv1d(x, weights)\n",
    "\n",
    "utils.test_conv_transpose1d_minimal(conv_transpose1d_minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_fractional_stride_1d` passed!\n"
     ]
    }
   ],
   "source": [
    "def fractional_stride_1d(x, stride: int = 1):\n",
    "    '''Returns a version of x suitable for transposed convolutions, i.e. \"spaced out\" with zeros\n",
    "    between its values.\n",
    "    This spacing only happens along the last dimension.\n",
    "\n",
    "    x: shape (batch, in_channels, width)\n",
    "\n",
    "    Example: \n",
    "        x = [[[1, 2, 3], [4, 5, 6]]]\n",
    "        stride = 2\n",
    "        output = [[[1, 0, 2, 0, 3], [4, 0, 5, 0, 6]]]\n",
    "    '''\n",
    "    new_shape = list(x.shape)\n",
    "    new_shape[-1] = x.shape[-1] + (x.shape[-1] - 1) * (stride - 1)\n",
    "\n",
    "    x_fstrided = t.zeros(new_shape, dtype=x.dtype, device=x.device)\n",
    "    x_fstrided[..., ::stride] = x\n",
    "    return x_fstrided\n",
    "\n",
    "utils.test_fractional_stride_1d(fractional_stride_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_conv_transpose1d` passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def conv_transpose1d(x, weights, stride: int = 1, padding: int = 0) -> t.Tensor:\n",
    "    '''Like torch's conv_transpose1d using bias=False and all other keyword arguments left at their\n",
    "    default values.\n",
    "\n",
    "    x: shape (batch, in_channels, width)\n",
    "    weights: shape (out_channels, in_channels, kernel_width)\n",
    "\n",
    "    Returns: shape (batch, out_channels, output_width)\n",
    "    '''\n",
    "    kernel_width = weights.shape[-1]\n",
    "    pad_size = kernel_width - 1 - padding\n",
    "\n",
    "    x = fractional_stride_1d(x, stride)\n",
    "    x = t.nn.functional.pad(x, (pad_size, pad_size))\n",
    "\n",
    "    weights = rearrange(weights, \"i o w -> o i w\")\n",
    "    weights = t.flip(weights, (-1,))\n",
    "\n",
    "    return t.nn.functional.conv1d(x, weights)\n",
    "\n",
    "utils.test_conv_transpose1d(conv_transpose1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IntOrPair = Union[int, Tuple[int, int]]\n",
    "Pair = Tuple[int, int]\n",
    "\n",
    "def force_pair(v: IntOrPair) -> Pair:\n",
    "    '''Convert v to a pair of int, if it isn't already.'''\n",
    "    if isinstance(v, tuple):\n",
    "        if len(v) != 2:\n",
    "            raise ValueError(v)\n",
    "        return (int(v[0]), int(v[1]))\n",
    "    elif isinstance(v, int):\n",
    "        return (v, v)\n",
    "    raise ValueError(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractional_stride_2d(x, stride_h: int, stride_w: int):\n",
    "    '''\n",
    "    Same as fractional_stride_1d, except we apply it along the last 2 dims of x (width and height).\n",
    "    '''\n",
    "    new_shape = list(x.shape)\n",
    "    new_shape[-2] = x.shape[-2] + (x.shape[-2] - 1) * (stride_h - 1)\n",
    "    new_shape[-1] = x.shape[-1] + (x.shape[-1] - 1) * (stride_w - 1)\n",
    "\n",
    "    x_fstrided = t.zeros(new_shape, dtype=x.dtype, device=x.device)\n",
    "    x_fstrided[..., ::stride_h, ::stride_w] = x\n",
    "    return x_fstrided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_conv_transpose2d` passed!\n"
     ]
    }
   ],
   "source": [
    "def conv_transpose2d(x, weights, stride: IntOrPair = 1, padding: IntOrPair = 0) -> t.Tensor:\n",
    "    '''Like torch's conv_transpose2d using bias=False\n",
    "\n",
    "    x: shape (batch, in_channels, height, width)\n",
    "    weights: shape (out_channels, in_channels, kernel_height, kernel_width)\n",
    "\n",
    "\n",
    "    Returns: shape (batch, out_channels, output_height, output_width)\n",
    "    '''\n",
    "    stride_h, stride_w = force_pair(stride)\n",
    "    padding_h, padding_w = force_pair(padding)\n",
    "\n",
    "    kernel_height, kernel_width = weights.shape[-2], weights.shape[-1]\n",
    "    pad_size_h = kernel_height - 1 - padding_h\n",
    "    pad_size_w = kernel_width - 1 - padding_w\n",
    "\n",
    "    x = fractional_stride_2d(x, stride_h, stride_w)\n",
    "    x = t.nn.functional.pad(x, (pad_size_w, pad_size_w, pad_size_h, pad_size_h))\n",
    "\n",
    "    weights = rearrange(weights, \"i o h w -> o i h w\")\n",
    "    weights = t.flip(weights, (-2, -1))\n",
    "\n",
    "    return t.nn.functional.conv2d(x, weights)\n",
    "\n",
    "utils.test_conv_transpose2d(conv_transpose2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_ConvTranspose2d` passed!\n"
     ]
    }
   ],
   "source": [
    "class ConvTranspose2d(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: IntOrPair,\n",
    "        stride: IntOrPair = 1,\n",
    "        padding: IntOrPair = 0\n",
    "    ):\n",
    "        '''\n",
    "        Same as torch.nn.ConvTranspose2d with bias=False.\n",
    "\n",
    "        Name your weight field `self.weight` for compatibility with the tests.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        kernel_h, kernel_w = force_pair(kernel_size)\n",
    "        self.stride = force_pair(stride)\n",
    "        self.padding = force_pair(padding)\n",
    "\n",
    "        sqrt_k = t.tensor(1 / (out_channels * kernel_w * kernel_h)).sqrt()\n",
    "        weight_shape = (in_channels, out_channels, kernel_h, kernel_w)\n",
    "        self.weight = nn.Parameter(t.FloatTensor(*weight_shape).uniform_(-sqrt_k, sqrt_k))\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        return conv_transpose2d(x, self.weight, stride=self.stride, padding=self.padding)\n",
    "\n",
    "utils.test_ConvTranspose2d(ConvTranspose2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_Tanh` passed.\n"
     ]
    }
   ],
   "source": [
    "class Tanh(nn.Module):\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        return (t.exp(2 * x) - 1) / (t.exp(2 * x) + 1)\n",
    "\n",
    "utils.test_Tanh(Tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_LeakyReLU` passed.\n"
     ]
    }
   ],
   "source": [
    "class LeakyReLU(nn.Module):\n",
    "    def __init__(self, negative_slope: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.negative_slope = negative_slope\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        return t.maximum(x, t.tensor(0)) + self.negative_slope * t.minimum(x, t.tensor(0))\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'negative_slope={self.negative_slope}'\n",
    "\n",
    "utils.test_LeakyReLU(LeakyReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_Sigmoid` passed.\n"
     ]
    }
   ],
   "source": [
    "class Sigmoid(nn.Module):\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        return 1 / (1 + t.exp(-x))\n",
    "\n",
    "utils.test_Sigmoid(Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84205e65a427e19ff91dd8a92d5a5f2cf0946f21e31957c24c166022f0f50b37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
