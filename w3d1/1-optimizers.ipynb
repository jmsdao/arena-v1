{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "\n",
    "from typing import Callable, Iterable, Tuple\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrocks_banana(x: t.Tensor, y: t.Tensor, a=1, b=100) -> t.Tensor:\n",
    "    return (a - x) ** 2 + b * (y - x**2) ** 2 + 1\n",
    "\n",
    "x_range = [-2, 2]\n",
    "y_range = [-1, 3]\n",
    "fig = utils.plot_fn(rosenbrocks_banana, x_range, y_range, log_scale=True, show_min=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_fn_with_sgd(fn: Callable, xy: t.Tensor, lr=0.001, momentum=0.98, n_iters: int = 100):\n",
    "    '''\n",
    "    Optimize the a given function starting from the specified point.\n",
    "\n",
    "    xy: shape (2,). The (x, y) starting point.\n",
    "    n_iters: number of steps.\n",
    "\n",
    "    Return: (n_iters, 2). The (x,y) BEFORE each step. So out[0] is the starting point.\n",
    "    '''\n",
    "    assert xy.requires_grad\n",
    "    xys = t.zeros((n_iters, 2))\n",
    "    optimizer = optim.SGD([xy], lr=lr, momentum=momentum)\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        xys[i] = xy.detach()\n",
    "        out = fn(xy[0], xy[1])\n",
    "        out.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return xys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = t.tensor([-1.5, 2.5], requires_grad=True)\n",
    "xys = opt_fn_with_sgd(rosenbrocks_banana, xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = t.tensor([-1.5, 2.5], requires_grad=True)\n",
    "x_range = [-2, 2]\n",
    "y_range = [-1, 3]\n",
    "\n",
    "fig = utils.plot_optimization_sgd(opt_fn_with_sgd, rosenbrocks_banana, xy, x_range, y_range, lr=0.001, momentum=0.98, show_min=True)\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing configuration:  {'lr': 0.1, 'momentum': 0.0, 'weight_decay': 0.0}\n",
      "\n",
      "Testing configuration:  {'lr': 0.1, 'momentum': 0.7, 'weight_decay': 0.0}\n",
      "\n",
      "Testing configuration:  {'lr': 0.1, 'momentum': 0.5, 'weight_decay': 0.0}\n",
      "\n",
      "Testing configuration:  {'lr': 0.1, 'momentum': 0.5, 'weight_decay': 0.05}\n",
      "\n",
      "Testing configuration:  {'lr': 0.2, 'momentum': 0.8, 'weight_decay': 0.05}\n"
     ]
    }
   ],
   "source": [
    "class SGD:\n",
    "    params: list\n",
    "\n",
    "    def __init__(self, params: Iterable[t.nn.parameter.Parameter], lr: float, momentum: float, weight_decay: float):\n",
    "        '''Implements SGD with momentum.\n",
    "\n",
    "        Like the PyTorch version, but assume nesterov=False, maximize=False, and dampening=0\n",
    "            https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
    "        '''\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.prev_grads = [t.zeros_like(p) for p in self.params]\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.grad = None\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def step(self) -> None:\n",
    "        for i in range(len(self.params)):\n",
    "            p = self.params[i]\n",
    "            new_grad = p.grad\n",
    "            prev_grad = self.prev_grads[i]\n",
    "\n",
    "            if self.weight_decay != 0:\n",
    "                new_grad = new_grad + self.weight_decay * p\n",
    "\n",
    "            if self.momentum != 0:\n",
    "                new_grad = self.momentum * prev_grad + new_grad\n",
    "\n",
    "            p.sub_(self.lr * new_grad)\n",
    "            self.prev_grads[i] = new_grad\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        # Should return something reasonable here, e.g. \"SGD(lr=lr, ...)\"\n",
    "        params_to_print = [\"lr\", \"momentum\", \"weight_decay\"]\n",
    "        params_string = ', '.join(f'{p}={getattr(self, p)}' for p in params_to_print)\n",
    "        return f'SGD({params_string})'\n",
    "\n",
    "utils.test_sgd(SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing configuration:  {'lr': 0.1, 'alpha': 0.9, 'eps': 0.001, 'weight_decay': 0.0, 'momentum': 0.0}\n",
      "\n",
      "Testing configuration:  {'lr': 0.1, 'alpha': 0.95, 'eps': 0.0001, 'weight_decay': 0.05, 'momentum': 0.0}\n",
      "\n",
      "Testing configuration:  {'lr': 0.1, 'alpha': 0.95, 'eps': 0.0001, 'weight_decay': 0.05, 'momentum': 0.5}\n",
      "\n",
      "Testing configuration:  {'lr': 0.1, 'alpha': 0.95, 'eps': 0.0001, 'weight_decay': 0.05, 'momentum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "class RMSprop:\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[t.nn.parameter.Parameter],\n",
    "        lr: float,\n",
    "        alpha: float,\n",
    "        weight_decay: float,\n",
    "        momentum: float,\n",
    "        eps: float = 1e-8,\n",
    "    ):\n",
    "        '''Implements RMSprop.\n",
    "\n",
    "        Like the PyTorch version, but assumes centered=False\n",
    "            https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop\n",
    "        '''\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "        self.vs = [t.zeros_like(p) for p in self.params]\n",
    "        self.bs = [t.zeros_like(p) for p in self.params]\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.grad = None\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def step(self) -> None:\n",
    "        lmbda = self.weight_decay\n",
    "        mu = self.momentum\n",
    "\n",
    "        for i in range(len(self.params)):\n",
    "            p = self.params[i]\n",
    "            new_grad = p.grad\n",
    "\n",
    "            if lmbda != 0:\n",
    "                new_grad = new_grad + lmbda * p\n",
    "\n",
    "            v = self.alpha * self.vs[i] + (1 - self.alpha) * new_grad ** 2\n",
    "            self.vs[i] = v\n",
    "\n",
    "            if mu > 0:\n",
    "                b = mu * self.bs[i] + new_grad / (t.sqrt(v) + self.eps)\n",
    "                p.sub_(self.lr * b)\n",
    "                self.bs[i] = b\n",
    "            else:\n",
    "                p.sub_(self.lr * new_grad / (t.sqrt(v) + self.eps))\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        params_to_print = [\"lr\", \"alpha\", \"weight_decay\", \"momentum\", \"eps\"]\n",
    "        params_string = ', '.join(f'{p}={getattr(self, p)}' for p in params_to_print)\n",
    "        return f'RMSprop({params_string})'\n",
    "\n",
    "\n",
    "utils.test_rmsprop(RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing configuration:  {'lr': 0.1, 'betas': (0.8, 0.95), 'eps': 0.001, 'weight_decay': 0.0}\n",
      "\n",
      "Testing configuration:  {'lr': 0.1, 'betas': (0.8, 0.9), 'eps': 0.001, 'weight_decay': 0.05}\n",
      "\n",
      "Testing configuration:  {'lr': 0.2, 'betas': (0.9, 0.95), 'eps': 0.01, 'weight_decay': 0.08}\n"
     ]
    }
   ],
   "source": [
    "class Adam:\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[t.nn.parameter.Parameter],\n",
    "        lr: float,\n",
    "        betas: Tuple[float, float],\n",
    "        weight_decay: float,\n",
    "        eps: float = 1e-8,\n",
    "    ):\n",
    "        '''Implements Adam.\n",
    "\n",
    "        Like the PyTorch version, but assumes amsgrad=False and maximize=False\n",
    "            https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam\n",
    "        '''\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "\n",
    "        self.t = 0\n",
    "        self.ms = [t.zeros_like(p) for p in self.params]\n",
    "        self.vs = [t.zeros_like(p) for p in self.params]\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        for param in self.params:\n",
    "            param.grad = None\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def step(self) -> None:\n",
    "        self.t += 1\n",
    "        b1 = self.betas[0]\n",
    "        b2 = self.betas[1]\n",
    "        lmbda = self.weight_decay\n",
    "        for i in range(len(self.params)):\n",
    "            p = self.params[i]\n",
    "            g = p.grad\n",
    "\n",
    "            if lmbda != 0:\n",
    "                g = g + lmbda * p\n",
    "            \n",
    "            m = b1 * self.ms[i] + (1 - b1) * g\n",
    "            v = b2 * self.vs[i] + (1 - b2) * g ** 2\n",
    "            self.ms[i] = m\n",
    "            self.vs[i] = v\n",
    "\n",
    "            mhat = m / (1 - b1 ** self.t)\n",
    "            vhat = v / (1 - b2 ** self.t)\n",
    "\n",
    "            p.sub_((self.lr * mhat) / (t.sqrt(vhat) + self.eps))\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        params_to_print = [\"lr\", \"betas\", \"eps\", \"weight_decay\"]\n",
    "        params_string = ', '.join(f'{p}={getattr(self, p)}' for p in params_to_print)\n",
    "        return f'Adam({params_string})'\n",
    "\n",
    "\n",
    "utils.test_adam(Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_fn(fn: Callable, xy: t.Tensor, optimizer_class, optimizer_kwargs, n_iters: int = 100):\n",
    "    '''Optimize the a given function starting from the specified point.\n",
    "\n",
    "    optimizer_class: one of the optimizers you've defined, either SGD, RMSprop, or Adam\n",
    "    optimzer_kwargs: keyword arguments passed to your optimiser (e.g. lr and weight_decay)\n",
    "    '''\n",
    "    assert xy.requires_grad\n",
    "    xys = t.zeros((n_iters, 2))\n",
    "    optimizer = optimizer_class([xy], **optimizer_kwargs)\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        xys[i] = xy.detach()\n",
    "        out = fn(xy[0], xy[1])\n",
    "        out.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return xys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = t.tensor([-1.5, 2.5], requires_grad=True)\n",
    "x_range = [-2, 2]\n",
    "y_range = [-1, 3]\n",
    "optimizers = [\n",
    "    (SGD, {'lr': 1e-3, 'weight_decay': 0.0, 'momentum': 0.98}),\n",
    "    (RMSprop, {'lr': 1e-1, 'alpha': 0.2, 'eps': 0.001, 'weight_decay': 0.0, 'momentum': 0.98}),\n",
    "    (Adam, {'lr': 2e-1, 'betas': (0.8, 0.8), 'eps': 0.001, 'weight_decay': 0.0}),\n",
    "]\n",
    "\n",
    "fig = utils.plot_optimization(opt_fn, rosenbrocks_banana, xy, optimizers, x_range, y_range, show_min=True)\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84205e65a427e19ff91dd8a92d5a5f2cf0946f21e31957c24c166022f0f50b37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
