{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.23.1 einops fancy-einsum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqPPLnRFH2yg",
        "outputId": "6b64aeef-839f-4eff-e1e5-780f3d5e68a3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym==0.23.1 in /usr/local/lib/python3.8/dist-packages (0.23.1)\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 10 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 20 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 30 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 40 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41 kB 343 kB/s \n",
            "\u001b[?25hCollecting fancy-einsum\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.23.1) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.23.1) (4.13.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym==0.23.1) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.23.1) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym==0.23.1) (3.10.0)\n",
            "Installing collected packages: fancy-einsum, einops\n",
            "Successfully installed einops-0.6.0 fancy-einsum-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/callummcdougall/arena-v1/raw/main/w6d2/utils.py\n",
        "!wget https://github.com/callummcdougall/arena-v1/raw/main/w6d2/solutions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXdSK79_whqb",
        "outputId": "408a51fd-7d4b-49f9-bd60-1d9744816e03"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-01 03:37:56--  https://github.com/callummcdougall/arena-v1/raw/main/w6d2/utils.py\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/callummcdougall/arena-v1/main/w6d2/utils.py [following]\n",
            "--2022-12-01 03:37:56--  https://raw.githubusercontent.com/callummcdougall/arena-v1/main/w6d2/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4242 (4.1K) [text/plain]\n",
            "Saving to: ‚Äòutils.py.1‚Äô\n",
            "\n",
            "\rutils.py.1            0%[                    ]       0  --.-KB/s               \rutils.py.1          100%[===================>]   4.14K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-01 03:37:56 (56.4 MB/s) - ‚Äòutils.py.1‚Äô saved [4242/4242]\n",
            "\n",
            "--2022-12-01 03:37:56--  https://github.com/callummcdougall/arena-v1/raw/main/w6d2/solutions.py\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/callummcdougall/arena-v1/main/w6d2/solutions.py [following]\n",
            "--2022-12-01 03:37:57--  https://raw.githubusercontent.com/callummcdougall/arena-v1/main/w6d2/solutions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11583 (11K) [text/plain]\n",
            "Saving to: ‚Äòsolutions.py‚Äô\n",
            "\n",
            "solutions.py        100%[===================>]  11.31K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-01 03:37:57 (100 MB/s) - ‚Äòsolutions.py‚Äô saved [11583/11583]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gettext import find\n",
        "from typing import Optional, Union, Tuple\n",
        "import numpy as np\n",
        "import gym\n",
        "import gym.spaces\n",
        "import gym.envs.registration\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image, ImageDraw\n",
        "import utils\n",
        "\n",
        "MAIN = __name__ == \"__main__\"\n",
        "Arr = np.ndarray"
      ],
      "metadata": {
        "id": "4o9skVkVwa9n"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "    def __init__(self, num_states: int, num_actions: int, start=0, terminal=None):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.start = start\n",
        "        self.terminal = np.array([], dtype=int) if terminal is None else terminal\n",
        "        (self.T, self.R) = self.build()\n",
        "\n",
        "    def build(self):\n",
        "        '''\n",
        "        Constructs the T and R tensors from the dynamics of the environment.\n",
        "        Outputs:\n",
        "            T : (num_states, num_actions, num_states) State transition probabilities\n",
        "            R : (num_states, num_actions, num_states) Reward function\n",
        "        '''\n",
        "        num_states = self.num_states\n",
        "        num_actions = self.num_actions\n",
        "        T = np.zeros((num_states, num_actions, num_states))\n",
        "        R = np.zeros((num_states, num_actions, num_states))\n",
        "        for s in range(num_states):\n",
        "            for a in range(num_actions):\n",
        "                (states, rewards, probs) = self.dynamics(s, a)\n",
        "                (all_s, all_r, all_p) = self.out_pad(states, rewards, probs)\n",
        "                T[s, a, all_s] = all_p\n",
        "                R[s, a, all_s] = all_r\n",
        "        return (T, R)\n",
        "\n",
        "    def dynamics(self, state: int, action: int) -> Tuple[Arr, Arr, Arr]:\n",
        "        '''\n",
        "        Computes the distribution over possible outcomes for a given state\n",
        "        and action.\n",
        "        Inputs:\n",
        "            state : int (index of state)\n",
        "            action : int (index of action)\n",
        "        Outputs:\n",
        "            states  : (m,) all the possible next states\n",
        "            rewards : (m,) rewards for each next state transition\n",
        "            probs   : (m,) likelihood of each state-reward pair\n",
        "        '''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def render(pi: Arr):\n",
        "        '''\n",
        "        Takes a policy pi, and draws an image of the behavior of that policy, if applicable.\n",
        "        Inputs:\n",
        "            pi : (num_actions,) a policy\n",
        "        Outputs:\n",
        "            None\n",
        "        '''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def out_pad(self, states: Arr, rewards: Arr, probs: Arr):\n",
        "        '''\n",
        "        Inputs:\n",
        "            states  : (m,) all the possible next states\n",
        "            rewards : (m,) rewards for each next state transition\n",
        "            probs   : (m,) likelihood of each state-reward pair\n",
        "        Outputs:\n",
        "            states  : (num_states,) all the next states\n",
        "            rewards : (num_states,) rewards for each next state transition\n",
        "            probs   : (num_states,) likelihood of each state-reward pair (including zero-prob outcomes.)\n",
        "        '''\n",
        "        out_s = np.arange(self.num_states)\n",
        "        out_r = np.zeros(self.num_states)\n",
        "        out_p = np.zeros(self.num_states)\n",
        "        for i in range(len(states)):\n",
        "            idx = states[i]\n",
        "            out_r[idx] += rewards[i]\n",
        "            out_p[idx] += probs[i]\n",
        "        return (out_s, out_r, out_p)"
      ],
      "metadata": {
        "id": "9JIBtCPfwbDL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Toy(Environment):\n",
        "    def dynamics(self, state: int, action: int):\n",
        "        (S0, SL, SR) = (0, 1, 2)\n",
        "        LEFT = 0\n",
        "        num_states = 3\n",
        "        num_actions = 2\n",
        "        assert 0 <= state < self.num_states and 0 <= action < self.num_actions\n",
        "        if state == S0:\n",
        "            if action == LEFT:\n",
        "                (next_state, reward) = (SL, 1)\n",
        "            else:\n",
        "                (next_state, reward) = (SR, 0)\n",
        "        elif state == SL:\n",
        "            (next_state, reward) = (S0, 0)\n",
        "        elif state == SR:\n",
        "            (next_state, reward) = (S0, 2)\n",
        "        return (np.array([next_state]), np.array([reward]), np.array([1]))\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(3, 2)\n",
        "\n",
        "toy = Toy()\n",
        "print(toy.T)\n",
        "print(toy.R)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2IbG7XS3At1",
        "outputId": "bdf50ca7-6c8c-42e7-9ccc-28db19e0fd8d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 1. 0.]\n",
            "  [0. 0. 1.]]\n",
            "\n",
            " [[1. 0. 0.]\n",
            "  [1. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0.]\n",
            "  [1. 0. 0.]]]\n",
            "[[[0. 1. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[2. 0. 0.]\n",
            "  [2. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Norvig(Environment):\n",
        "    def dynamics(self, state: int, action: int) -> Tuple[Arr, Arr, Arr]:\n",
        "        def state_index(state):\n",
        "            assert 0 <= state[0] < self.width and 0 <= state[1] < self.height, print(state)\n",
        "            pos = state[0] + state[1] * self.width\n",
        "            assert 0 <= pos < self.num_states, print(state, pos)\n",
        "            return pos\n",
        "\n",
        "        pos = self.states[state]\n",
        "        move = self.actions[action]\n",
        "        if state in self.terminal or state in self.walls:\n",
        "            return (np.array([state]), np.array([0]), np.array([1]))\n",
        "        out_probs = np.zeros(self.num_actions) + 0.1\n",
        "        out_probs[action] = 0.7\n",
        "        out_states = np.zeros(self.num_actions, dtype=int) + self.num_actions\n",
        "        out_rewards = np.zeros(self.num_actions) + self.penalty\n",
        "        new_states = [pos + x for x in self.actions]\n",
        "        for (i, s_new) in enumerate(new_states):\n",
        "            if not (0 <= s_new[0] < self.width and 0 <= s_new[1] < self.height):\n",
        "                out_states[i] = state\n",
        "                continue\n",
        "            new_state = state_index(s_new)\n",
        "            if new_state in self.walls:\n",
        "                out_states[i] = state\n",
        "            else:\n",
        "                out_states[i] = new_state\n",
        "            for idx in range(len(self.terminal)):\n",
        "                if new_state == self.terminal[idx]:\n",
        "                    out_rewards[i] = self.goal_rewards[idx]\n",
        "        return (out_states, out_rewards, out_probs)\n",
        "\n",
        "    def render(self, pi: Arr):\n",
        "        assert len(pi) == self.num_states\n",
        "        emoji = [\"‚¨ÜÔ∏è\", \"‚û°Ô∏è\", \"‚¨áÔ∏è\", \"‚¨ÖÔ∏è\"]\n",
        "        grid = [emoji[act] for act in pi]\n",
        "        grid[3] = \"üü©\"\n",
        "        grid[7] = \"üü•\"\n",
        "        grid[5] = \"‚¨õ\"\n",
        "        print(str(grid[0:4]) + \"\\n\" + str(grid[4:8]) + \"\\n\" + str(grid[8:]))\n",
        "\n",
        "    def __init__(self, penalty=-0.04):\n",
        "        self.height = 3\n",
        "        self.width = 4\n",
        "        self.penalty = penalty\n",
        "        num_states = self.height * self.width\n",
        "        num_actions = 4\n",
        "        self.states = np.array([[x, y] for y in range(self.height) for x in range(self.width)])\n",
        "        self.actions = np.array([[0, -1], [1, 0], [0, 1], [-1, 0]])\n",
        "        self.dim = (self.height, self.width)\n",
        "        terminal = np.array([3, 7], dtype=int)\n",
        "        self.walls = np.array([5], dtype=int)\n",
        "        self.goal_rewards = np.array([1.0, -1])\n",
        "        super().__init__(num_states, num_actions, start=8, terminal=terminal)"
      ],
      "metadata": {
        "id": "FIjF5VcY3Aw-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_eval_numerical(env: Environment, pi: Arr, gamma=0.99, eps=1e-08, max_iterations=10_000) -> Arr:\n",
        "    '''\n",
        "    Numerically evaluates the value of a given policy by iterating the Bellman equation\n",
        "    Inputs:\n",
        "        env: Environment\n",
        "        pi : shape (num_states,) - The policy to evaluate\n",
        "        gamma: float - Discount factor\n",
        "        eps  : float - Tolerance\n",
        "    Outputs:\n",
        "        value : float (num_states,) - The value function for policy pi\n",
        "    '''\n",
        "    states = np.arange(env.num_states)\n",
        "    transitions = env.T[states, pi, :]\n",
        "    rewards = env.R[states, pi, :]\n",
        "    V_old = np.zeros_like(pi)\n",
        "\n",
        "    done = False\n",
        "    iteration = 0\n",
        "    while not done:\n",
        "        V_new = (transitions * (rewards + gamma * V_old)).sum(axis=1)\n",
        "        # Done conditions\n",
        "        iteration += 1\n",
        "        if np.abs(V_new - V_old).max() < eps:\n",
        "            done = True\n",
        "            print(f\"Converged in {iteration} steps\")\n",
        "        elif iteration > max_iterations:\n",
        "            done = True\n",
        "            print(f\"Failed to converge after {iteration} steps\")\n",
        "        else:\n",
        "            V_old = V_new\n",
        "\n",
        "    return V_old\n",
        "\n",
        "\n",
        "utils.test_policy_eval(policy_eval_numerical, exact=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIgAnkVD6CNk",
        "outputId": "d2be59c9-ed2b-4d31-fae0-37a201c3aa8e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged in 107 steps.\n",
            "Converged in 108 steps\n",
            "Converged in 101 steps.\n",
            "Converged in 102 steps\n",
            "Converged in 143 steps.\n",
            "Converged in 144 steps\n",
            "Converged in 145 steps.\n",
            "Converged in 146 steps\n",
            "Converged in 115 steps.\n",
            "Converged in 116 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_eval_exact(env: Environment, pi: Arr, gamma=0.99) -> Arr:\n",
        "    states = np.arange(env.num_states)\n",
        "    P = env.T[states, pi, :]\n",
        "    R = env.R[states, pi, :]\n",
        "\n",
        "    I = np.eye(P.shape[0])\n",
        "    r = (P * R).sum(axis=1)\n",
        "\n",
        "    v = np.linalg.inv(I - gamma * P) @ r\n",
        "    return v\n",
        "\n",
        "\n",
        "utils.test_policy_eval(policy_eval_exact, exact=True)"
      ],
      "metadata": {
        "id": "jS0RDpm4_Z4m"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(env: Environment, V: Arr, gamma=0.99) -> Arr:\n",
        "    '''\n",
        "    Inputs:\n",
        "        env: Environment\n",
        "        V  : (num_states,) value of each state following some policy pi\n",
        "    Outputs:\n",
        "        pi_better : vector (num_states,) of actions representing a new policy obtained via policy iteration\n",
        "    '''\n",
        "    T = env.T\n",
        "    R = env.R\n",
        "    state_action_values = (T * (R + gamma * V)).sum(axis=2)\n",
        "    pi_better = state_action_values.argmax(axis=1)\n",
        "\n",
        "    return pi_better\n",
        "\n",
        "\n",
        "utils.test_policy_improvement(policy_improvement)"
      ],
      "metadata": {
        "id": "E9mftMSTwbK-"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_optimal_policy(env: Environment, gamma=0.99, max_iterations=10_000):\n",
        "    '''\n",
        "    Inputs:\n",
        "        env: environment\n",
        "    Outputs:\n",
        "        pi : (num_states,) int, of actions represeting an optimal policy\n",
        "    '''\n",
        "    pi_old = np.zeros(shape=env.num_states, dtype=int)\n",
        "    converged = False\n",
        "    for i in range(max_iterations):\n",
        "        V = policy_eval_exact(env, pi_old, gamma=gamma)\n",
        "        pi_new = policy_improvement(env, V, gamma=gamma)\n",
        "\n",
        "        # Check for convergence\n",
        "        if all(pi_new == pi_old):\n",
        "            converged = True\n",
        "            break\n",
        "\n",
        "        pi_old = pi_new\n",
        "\n",
        "    if not converged:\n",
        "        print(f\"Did not converge after {max_iterations} steps.\")\n",
        "\n",
        "    return pi_new\n",
        "\n",
        "\n",
        "\n",
        "utils.test_find_optimal_policy(find_optimal_policy)\n",
        "penalty = -0.04\n",
        "norvig = Norvig(penalty)\n",
        "pi_opt = find_optimal_policy(norvig, gamma=0.99)\n",
        "norvig.render(pi_opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV3rkl4RKvqT",
        "outputId": "0c8d33ae-8801-429c-9e6e-aeab6eeb10b9"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['‚û°Ô∏è', '‚û°Ô∏è', '‚û°Ô∏è', 'üü©']\n",
            "['‚¨ÜÔ∏è', '‚¨õ', '‚¨ÜÔ∏è', 'üü•']\n",
            "['‚¨ÜÔ∏è', '‚¨ÖÔ∏è', '‚¨ÜÔ∏è', '‚¨ÖÔ∏è']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xKoYmSkRGtio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3KLUGqQnGtkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7NZwGGPkGtnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4mpWE9tsGtp-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}