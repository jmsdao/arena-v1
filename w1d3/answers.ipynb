{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "\n",
    "from fancy_einsum import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, num_heads: int):\n",
    "    '''\n",
    "    Implements multihead masked attention on the matrices Q, K and V.\n",
    "\n",
    "    Q: shape (batch, seq, nheads*headsize)\n",
    "    K: shape (batch, seq, nheads*headsize)\n",
    "    V: shape (batch, seq, nheads*headsize)\n",
    "    '''\n",
    "    batch = Q.shape[0]\n",
    "    seq_len = Q.shape[1]\n",
    "    headsize = Q.shape[2] // num_heads\n",
    "\n",
    "    Q = Q.reshape(batch, seq_len, num_heads, headsize)\n",
    "    K = K.reshape(batch, seq_len, num_heads, headsize)\n",
    "    V = V.reshape(batch, seq_len, num_heads, headsize)\n",
    "\n",
    "    scale = t.sqrt(t.tensor(K.shape[-1]).type(t.float32))\n",
    "    raw_attention_filter = einsum('b sl_Q nh hs, b sl_K nh hs -> b nh sl_Q sl_K', Q, K)\n",
    "    mask_filter = t.triu(t.full_like(raw_attention_filter, -t.inf), 1)\n",
    "    masked_attention_filter = t.softmax((raw_attention_filter + mask_filter) / scale, dim=-1)\n",
    "    attention_values = einsum('b nh sl_Q sl_K, b sl_K nh hs -> b sl_Q nh hs', masked_attention_filter, V)\n",
    "    return attention_values.reshape(batch, seq_len, num_heads * headsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadMaskedAttention(nn.Module):\n",
    "    W_QKV: nn.Linear\n",
    "    W_O: nn.Linear\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        assert hidden_size % num_heads == 0, \"num_heads should be divisible by hidden_size\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.W_QKV = nn.Linear(hidden_size, 3 * hidden_size, bias=False)\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        headsize = self.hidden_size // self.num_heads\n",
    "\n",
    "        QKV = self.W_QKV(x)        \n",
    "        Q = QKV[..., :self.hidden_size]\n",
    "        K = QKV[..., self.hidden_size:2*self.hidden_size]\n",
    "        V = QKV[..., 2*self.hidden_size:3*self.hidden_size]\n",
    "        attention_values = multihead_masked_attention(Q, K, V, self.num_heads)\n",
    "        return self.W_O(attention_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.9091,  -1.2757,   0.7524,  -0.4398,   0.5692,  -0.0323],\n",
       "         [  1.0873,  -1.7496,   0.8778,  -0.5991,   1.0275,   0.3806],\n",
       "         [  1.7786,  -3.5476,   1.3684,  -1.2170,   2.8066,   2.0091]],\n",
       "\n",
       "        [[  0.2038, -13.1862,  -1.1567,   0.3344,  -1.8587, -11.3195],\n",
       "         [  0.7182, -14.5300,  -0.7923,  -0.1254,  -0.5352, -10.1120],\n",
       "         [  1.2383, -15.8890,  -0.4238,  -0.5903,   0.8032,  -8.8908]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.manual_seed(420)\n",
    "m = MultiheadMaskedAttention(6, 2)\n",
    "x = t.linspace(0, 42, 2 * 3 * 6).reshape(2, 3, 6)\n",
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84205e65a427e19ff91dd8a92d5a5f2cf0946f21e31957c24c166022f0f50b37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
