{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "\n",
    "from einops import reduce, rearrange\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_batchnorm2d_module` passed!\n",
      "All tests in `test_batchnorm2d_forward` passed!\n",
      "All tests in `test_batchnorm2d_running_mean` passed!\n"
     ]
    }
   ],
   "source": [
    "class BatchNorm2d(nn.Module):\n",
    "    running_mean: t.Tensor         # shape: (num_features,)\n",
    "    running_var: t.Tensor          # shape: (num_features,)\n",
    "    num_batches_tracked: t.Tensor  # shape: ()\n",
    "\n",
    "    def __init__(self, num_features: int, eps=1e-05, momentum=0.1):\n",
    "        '''Like nn.BatchNorm2d with track_running_stats=True and affine=True.\n",
    "\n",
    "        Name the learnable affine parameters `weight` and `bias` in that order.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.weight = nn.Parameter(t.ones(num_features))\n",
    "        self.bias = nn.Parameter(t.zeros(num_features))\n",
    "        \n",
    "        self.register_buffer(\"running_mean\", t.zeros(num_features))\n",
    "        self.register_buffer(\"running_var\", t.ones(num_features))\n",
    "        self.register_buffer(\"num_batches_tracked\", t.tensor(0))\n",
    "        \n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Normalize each channel.\n",
    "\n",
    "        Compute the variance using `torch.var(x, unbiased=False)`\n",
    "        Hint: you may also find it helpful to use the argument `keepdim`.\n",
    "\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, channels, height, width)\n",
    "        '''\n",
    "        if self.training:\n",
    "            self.num_batches_tracked += 1\n",
    "\n",
    "            mean = t.mean(x, dim=(0, 2, 3), keepdim=True)\n",
    "            var = t.var(x, dim=(0, 2, 3), unbiased=False, keepdim=True)\n",
    "\n",
    "            self.running_mean = (1 - self.momentum) * mean.squeeze() + \\\n",
    "                                self.momentum * self.running_mean\n",
    "            self.running_var = (1 - self.momentum) * var.squeeze() + \\\n",
    "                                self.momentum * self.running_var\n",
    "        else:\n",
    "            mean = rearrange(self.running_mean, \"c -> 1 c 1 1\")\n",
    "            var = rearrange(self.running_var, \"c -> 1 c 1 1\")\n",
    "\n",
    "        weight = rearrange(self.weight, \"c -> 1 c 1 1\")\n",
    "        bias = rearrange(self.bias, \"c -> 1 c 1 1\")\n",
    "        return ((x - mean) / t.sqrt(var + self.eps)) * weight + bias\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \", \".join(\n",
    "            [f\"{key}={getattr(self, key)}\" for key in [\"num_features\", \"eps\", \"momentum\"]]\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    utils.test_batchnorm2d_module(BatchNorm2d)\n",
    "    utils.test_batchnorm2d_forward(BatchNorm2d)\n",
    "    utils.test_batchnorm2d_running_mean(BatchNorm2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePool(nn.Module):\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, channels, height, width)\n",
    "        Return: shape (batch, channels)\n",
    "        '''\n",
    "        return reduce(x, 'b c h w -> b c 1 1', 'mean')\n",
    "\n",
    "# Testing\n",
    "if __name__ == \"__main__\":\n",
    "    for c in range(1, 10):\n",
    "        x = t.rand(4, c, c+2, c+2)\n",
    "        t.testing.assert_close(AveragePool()(x), nn.AvgPool2d((c+2,c+2))(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int, first_stride=1):\n",
    "        '''A single residual block with optional downsampling.\n",
    "\n",
    "        For compatibility with the pretrained model, declare the left side branch first\n",
    "        a `Sequential`.\n",
    "\n",
    "        If first_stride is > 1, this means the optional (conv + bn) should be present on the\n",
    "        right branch. Declare it second using another `Sequential`.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        if first_stride == 1:\n",
    "            assert in_feats == out_feats, \\\n",
    "                \"Invalid ResBlock: if first_stride==1, we require in_feats == out_feats\"\n",
    "\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_feats, out_feats,\n",
    "                kernel_size=3, stride=first_stride, padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_feats),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_feats, out_feats, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_feats)\n",
    "        )\n",
    "\n",
    "        if first_stride <= 1:\n",
    "            self.right = nn.Identity()\n",
    "        else:\n",
    "            self.right = nn.Sequential(\n",
    "                nn.Conv2d(in_feats, out_feats, kernel_size=1, stride=first_stride, bias=False),\n",
    "                nn.BatchNorm2d(out_feats)\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Compute the forward pass.\n",
    "\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / stride, width / stride)\n",
    "\n",
    "        If no downsampling block is present, the addition should just add the left branch's output\n",
    "        to the input.\n",
    "        '''\n",
    "        return self.relu(self.left(x) + self.right(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockGroup(nn.Module):\n",
    "    def __init__(self, n_blocks: int, in_feats: int, out_feats: int, first_stride=1):\n",
    "        '''An n_blocks-long sequence of ResidualBlock where only the first block uses\n",
    "        the provided stride.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            ResidualBlock(in_feats, out_feats, first_stride),\n",
    "            *[ResidualBlock(out_feats, out_feats, 1) for _ in range(n_blocks-1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Compute the forward pass.\n",
    "        x: shape (batch, in_feats, height, width)\n",
    "\n",
    "        Return: shape (batch, out_feats, height / first_stride, width / first_stride)\n",
    "        '''\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_blocks_per_group=[3, 4, 6, 3],\n",
    "        out_features_per_group=[64, 128, 256, 512],\n",
    "        first_strides_per_group=[1, 2, 2, 2],\n",
    "        n_classes=1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert (\n",
    "            len(n_blocks_per_group) == len(out_features_per_group) == len(first_strides_per_group)\n",
    "        ), \"BlockGroup params need to properly defined.\"\n",
    "\n",
    "        in_feat = 64\n",
    "        in_features_per_group = [in_feat] + out_features_per_group[:-1]\n",
    "        zipped_params = zip(\n",
    "            n_blocks_per_group,\n",
    "            in_features_per_group,\n",
    "            out_features_per_group,\n",
    "            first_strides_per_group\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, in_feat, kernel_size=7, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(in_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            *[BlockGroup(*params) for params in zipped_params],\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(out_features_per_group[-1], n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, channels, height, width)\n",
    "\n",
    "        Return: shape (batch, n_classes)\n",
    "        '''\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_resnet = ResNet34()\n",
    "\n",
    "from torchvision.models import resnet34\n",
    "pt_resnet = resnet34()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weights(myresnet: ResNet34, pretrained_resnet: torchvision.models.resnet.ResNet) -> ResNet34:\n",
    "    '''Copy over the weights of `pretrained_resnet` to your resnet.'''\n",
    "\n",
    "    mydict = myresnet.state_dict().items()\n",
    "    pretraineddict = pretrained_resnet.state_dict().items()\n",
    "\n",
    "    # Check the number of params/buffers is correct\n",
    "    assert len(mydict) == len(pretraineddict), \\\n",
    "        \"Number of layers is wrong. Have you done the prev step correctly?\"\n",
    "\n",
    "    # Initialise an empty dictionary to store the correct key-value pairs\n",
    "    state_dict_to_load = {}\n",
    "\n",
    "    for (mykey, myvalue), (pretrainedkey, pretrainedvalue) in zip(mydict, pretraineddict):\n",
    "        state_dict_to_load[mykey] = pretrainedvalue\n",
    "\n",
    "    myresnet.load_state_dict(state_dict_to_load)\n",
    "\n",
    "    return myresnet\n",
    "\n",
    "my_resnet = copy_weights(my_resnet, pt_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84205e65a427e19ff91dd8a92d5a5f2cf0946f21e31957c24c166022f0f50b37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
