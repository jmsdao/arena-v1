{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (I) NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 2339.975\n",
      "loss = 1599.035\n",
      "loss = 1100.922\n",
      "loss = 765.791\n",
      "loss = 540.099\n",
      "loss = 387.933\n",
      "loss = 285.196\n",
      "loss = 215.717\n",
      "loss = 168.634\n",
      "loss = 136.653\n",
      "loss = 114.866\n",
      "loss = 99.975\n",
      "loss = 89.757\n",
      "loss = 82.713\n",
      "loss = 77.832\n",
      "loss = 74.428\n",
      "loss = 72.039\n",
      "loss = 70.349\n",
      "loss = 69.144\n",
      "loss = 68.276\n",
      "loss = 67.646\n",
      "loss = 67.184\n",
      "loss = 66.841\n",
      "loss = 66.585\n",
      "loss = 66.391\n",
      "loss = 66.243\n",
      "loss = 66.129\n",
      "loss = 66.041\n",
      "loss = 65.972\n",
      "loss = 65.917\n",
      "loss = 65.874\n",
      "loss = 65.840\n",
      "loss = 65.812\n",
      "loss = 65.790\n",
      "loss = 65.773\n",
      "loss = 65.758\n",
      "loss = 65.747\n",
      "loss = 65.737\n",
      "loss = 65.730\n",
      "loss = 65.723\n"
     ]
    }
   ],
   "source": [
    "NUM_FREQUENCIES = 2\n",
    "TARGET_FUNC = lambda x: 1 * (x > 1)\n",
    "TOTAL_STEPS = 4000\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "x = np.linspace(-np.pi, np.pi, 2000)\n",
    "y = TARGET_FUNC(x)\n",
    "\n",
    "x_cos = np.array([np.cos(n*x) for n in range(1, NUM_FREQUENCIES+1)])\n",
    "x_sin = np.array([np.sin(n*x) for n in range(1, NUM_FREQUENCIES+1)])\n",
    "\n",
    "a_0 = np.random.randn()\n",
    "A_n = np.random.randn(NUM_FREQUENCIES)\n",
    "B_n = np.random.randn(NUM_FREQUENCIES)\n",
    "\n",
    "y_pred_list = []\n",
    "coeffs_list = []\n",
    "\n",
    "for step in range(TOTAL_STEPS):\n",
    "\n",
    "    # compute `y_pred` using your coeffs, and the terms `x_cos`, `x_sin`\n",
    "    y_pred = (0.5 * a_0) + (A_n @ x_cos) + (B_n @ x_sin)\n",
    "\n",
    "    # compute `loss`, which is the sum of squared error between `y` and `y_pred`\n",
    "    sq_error = (y - y_pred) ** 2\n",
    "    loss = sq_error.sum()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"loss = {loss:.3f}\")\n",
    "        coeffs_list.append([a_0, A_n.copy(), B_n.copy()])\n",
    "        y_pred_list.append(y_pred)\n",
    "\n",
    "    # compute gradients of coeffs with respect to `loss`\n",
    "    dL_dy_pred = 2 * (y_pred - y)\n",
    "    dL_da_0 = (0.5 * dL_dy_pred).sum()\n",
    "    dL_dA_n = (dL_dy_pred * x_cos).sum(axis=1)\n",
    "    dL_dB_n = (dL_dy_pred * x_sin).sum(axis=1)\n",
    "\n",
    "    # update weights using gradient descent (using the parameter `LEARNING_RATE`)\n",
    "    a_0 -= dL_da_0 * LEARNING_RATE\n",
    "    A_n -= dL_dA_n * LEARNING_RATE\n",
    "    B_n -= dL_dB_n * LEARNING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.visualise_fourier_coeff_convergence(x, y, y_pred_list, coeffs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (II) PyTorch & Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 2529.620\n",
      "loss = 1742.727\n",
      "loss = 1210.694\n",
      "loss = 850.312\n",
      "loss = 605.660\n",
      "loss = 439.137\n",
      "loss = 325.438\n",
      "loss = 247.522\n",
      "loss = 193.898\n",
      "loss = 156.809\n",
      "loss = 131.011\n",
      "loss = 112.951\n",
      "loss = 100.215\n",
      "loss = 91.163\n",
      "loss = 84.674\n",
      "loss = 79.978\n",
      "loss = 76.548\n",
      "loss = 74.017\n",
      "loss = 72.130\n",
      "loss = 70.710\n",
      "loss = 69.631\n",
      "loss = 68.804\n",
      "loss = 68.164\n",
      "loss = 67.666\n",
      "loss = 67.274\n",
      "loss = 66.965\n",
      "loss = 66.719\n",
      "loss = 66.523\n",
      "loss = 66.366\n",
      "loss = 66.240\n",
      "loss = 66.138\n",
      "loss = 66.056\n",
      "loss = 65.989\n",
      "loss = 65.935\n",
      "loss = 65.891\n",
      "loss = 65.855\n",
      "loss = 65.826\n",
      "loss = 65.802\n",
      "loss = 65.783\n",
      "loss = 65.767\n"
     ]
    }
   ],
   "source": [
    "NUM_FREQUENCIES = 2\n",
    "TARGET_FUNC = lambda x: 1 * (x > 1)\n",
    "TOTAL_STEPS = 4000\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "x = torch.linspace(-torch.pi, torch.pi, 2000)\n",
    "y = TARGET_FUNC(x)\n",
    "\n",
    "x_cos = torch.stack([torch.cos(n*x) for n in range(1, NUM_FREQUENCIES+1)])\n",
    "x_sin = torch.stack([torch.sin(n*x) for n in range(1, NUM_FREQUENCIES+1)])\n",
    "\n",
    "a_0 = torch.randn(1)\n",
    "A_n = torch.randn(NUM_FREQUENCIES)\n",
    "B_n = torch.randn(NUM_FREQUENCIES)\n",
    "\n",
    "y_pred_list = []\n",
    "coeffs_list = []\n",
    "\n",
    "for step in range(TOTAL_STEPS):\n",
    "\n",
    "    # compute `y_pred` using your coeffs, and the terms `x_cos`, `x_sin`\n",
    "    y_pred = (0.5 * a_0) + (A_n @ x_cos) + (B_n @ x_sin)\n",
    "\n",
    "    # compute `loss`, which is the sum of squared error between `y` and `y_pred`\n",
    "    sq_error = (y - y_pred) ** 2\n",
    "    loss = sq_error.sum()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"loss = {loss:.3f}\")\n",
    "        coeffs_list.append([a_0.item(), A_n.clone().numpy(), B_n.clone().numpy()])\n",
    "        y_pred_list.append(y_pred)\n",
    "\n",
    "    # compute gradients of coeffs with respect to `loss`\n",
    "    dL_dy_pred = 2 * (y_pred - y)\n",
    "    dL_da_0 = (0.5 * dL_dy_pred).sum()\n",
    "    dL_dA_n = (dL_dy_pred * x_cos).sum(axis=1)\n",
    "    dL_dB_n = (dL_dy_pred * x_sin).sum(axis=1)\n",
    "\n",
    "    # update weights using gradient descent (using the parameter `LEARNING_RATE`)\n",
    "    a_0 -= dL_da_0 * LEARNING_RATE\n",
    "    A_n -= dL_dA_n * LEARNING_RATE\n",
    "    B_n -= dL_dB_n * LEARNING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.visualise_fourier_coeff_convergence(x, y, y_pred_list, coeffs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (III) Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grads computed successfully!\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2, dtype=torch.float, requires_grad=True)\n",
    "b = torch.tensor(3, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "Q = 3*a**3 - b**2\n",
    "Q.backward()\n",
    "\n",
    "assert 9*a**2 == a.grad\n",
    "assert -2*b == b.grad\n",
    "print(\"Grads computed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 5217.872\n",
      "loss = 3609.214\n",
      "loss = 2514.821\n",
      "loss = 1767.991\n",
      "loss = 1256.489\n",
      "loss = 904.672\n",
      "loss = 661.495\n",
      "loss = 492.460\n",
      "loss = 374.206\n",
      "loss = 290.881\n",
      "loss = 231.703\n",
      "loss = 189.309\n",
      "loss = 158.659\n",
      "loss = 136.284\n",
      "loss = 119.787\n",
      "loss = 107.502\n",
      "loss = 98.261\n",
      "loss = 91.244\n",
      "loss = 85.866\n",
      "loss = 81.709\n",
      "loss = 78.471\n",
      "loss = 75.930\n",
      "loss = 73.924\n",
      "loss = 72.331\n",
      "loss = 71.061\n",
      "loss = 70.043\n",
      "loss = 69.224\n",
      "loss = 68.564\n",
      "loss = 68.030\n",
      "loss = 67.598\n",
      "loss = 67.247\n",
      "loss = 66.961\n",
      "loss = 66.729\n",
      "loss = 66.540\n",
      "loss = 66.386\n",
      "loss = 66.260\n",
      "loss = 66.157\n",
      "loss = 66.073\n",
      "loss = 66.004\n",
      "loss = 65.948\n"
     ]
    }
   ],
   "source": [
    "NUM_FREQUENCIES = 2\n",
    "TARGET_FUNC = lambda x: 1 * (x > 1)\n",
    "TOTAL_STEPS = 4000\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "x = torch.linspace(-torch.pi, torch.pi, 2000)\n",
    "y = TARGET_FUNC(x)\n",
    "\n",
    "x_cos = torch.stack([torch.cos(n*x) for n in range(1, NUM_FREQUENCIES+1)])\n",
    "x_sin = torch.stack([torch.sin(n*x) for n in range(1, NUM_FREQUENCIES+1)])\n",
    "\n",
    "a_0 = torch.randn(1, requires_grad=True)\n",
    "A_n = torch.randn(NUM_FREQUENCIES, requires_grad=True)\n",
    "B_n = torch.randn(NUM_FREQUENCIES, requires_grad=True)\n",
    "\n",
    "y_pred_list = []\n",
    "coeffs_list = []\n",
    "\n",
    "for step in range(TOTAL_STEPS):\n",
    "\n",
    "    # compute `y_pred` using your coeffs, and the terms `x_cos`, `x_sin`\n",
    "    y_pred = (0.5 * a_0) + (A_n @ x_cos) + (B_n @ x_sin)\n",
    "\n",
    "    # compute `loss`, which is the sum of squared error between `y` and `y_pred`\n",
    "    sq_error = (y - y_pred) ** 2\n",
    "    loss = sq_error.sum()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"loss = {loss:.3f}\")\n",
    "        coeffs_list.append([a_0.item(), A_n.clone().detach().numpy(), B_n.clone().detach().numpy()])\n",
    "        y_pred_list.append(y_pred.detach().numpy())\n",
    "\n",
    "    # compute gradients of coeffs with respect to `loss`\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights using gradient descent (using the parameter `LEARNING_RATE`)\n",
    "    with torch.no_grad():\n",
    "        a_0 -= a_0.grad * LEARNING_RATE\n",
    "        A_n -= A_n.grad * LEARNING_RATE\n",
    "        B_n -= B_n.grad * LEARNING_RATE\n",
    "        a_0.grad = None\n",
    "        A_n.grad = None\n",
    "        B_n.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.visualise_fourier_coeff_convergence(x, y, y_pred_list, coeffs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84205e65a427e19ff91dd8a92d5a5f2cf0946f21e31957c24c166022f0f50b37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
