{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Iterable, Iterator, Optional, Union, Tuple, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arr = np.ndarray\n",
    "\n",
    "def log_back(grad_out: Arr, out: Arr, x: Arr) -> Arr:\n",
    "    '''Backwards function for f(x) = log(x)\n",
    "\n",
    "    grad_out: gradient of some loss wrt out\n",
    "    out: the output of np.log(x)\n",
    "    x: the input of np.log\n",
    "\n",
    "    Return: gradient of the given loss wrt x\n",
    "    '''\n",
    "    return grad_out * (1 / x)\n",
    "\n",
    "\n",
    "def unbroadcast(broadcasted: Arr, original: Arr) -> Arr:\n",
    "    '''Sum 'broadcasted' until it has the shape of 'original'.\n",
    "\n",
    "    broadcasted: An array that was formerly of the same shape of 'original'\n",
    "    and was expanded by broadcasting rules.\n",
    "    '''\n",
    "    # Sum and remove dimensions that were prepended to the front of the original shape.\n",
    "    n_dims_prepended = len(broadcasted.shape) - len(original.shape)\n",
    "    unbroadcasted = broadcasted.sum(axis=tuple(range(n_dims_prepended)))\n",
    "\n",
    "    # Sum dimensions that were originally 1 back to the size 1 (using keepdims=True).\n",
    "    for dim, os in enumerate(original.shape):\n",
    "        if os == 1:\n",
    "            unbroadcasted = unbroadcasted.sum(axis=dim, keepdims=True)\n",
    "    \n",
    "    return unbroadcasted\n",
    "\n",
    "\n",
    "def multiply_back0(grad_out: Arr, out: Arr, x: Arr, y: Union[Arr, float]) -> Arr:\n",
    "    \"\"\"Backwards function for x * y wrt argument 0 aka x.\"\"\"\n",
    "    if not isinstance(y, Arr):\n",
    "        y = np.array(y)\n",
    "    return unbroadcast(y * grad_out, x)\n",
    "\n",
    "\n",
    "def multiply_back1(grad_out: Arr, out: Arr, x: Union[Arr, float], y: Arr) -> Arr:\n",
    "    \"\"\"Backwards function for x * y wrt argument 1 aka y.\"\"\"\n",
    "    if not isinstance(x, Arr):\n",
    "        x = np.array(x)\n",
    "    return unbroadcast(x * grad_out, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Recipe:\n",
    "    '''Extra information necessary to run backpropagation. You don't need to modify this.'''\n",
    "\n",
    "    func: Callable\n",
    "    \"\"\"\n",
    "    The 'inner' NumPy function that does the actual forward computation.\n",
    "    Note, we call it 'inner' to distinguish it from the wrapper we'll create for it later on.\n",
    "    \"\"\"\n",
    "    args: tuple\n",
    "    \"\"\"\n",
    "    The input arguments passed to func.\n",
    "    For instance, if func was np.sum then args would be a length-1 tuple containing the\n",
    "    tensor to be summed.\n",
    "    \"\"\"\n",
    "    kwargs: Dict[str, Any]\n",
    "    \"\"\"\n",
    "    Keyword arguments passed to func.\n",
    "    For instance, if func was np.sum then kwargs might contain 'dim' and 'keepdims'.\n",
    "    \"\"\"\n",
    "    parents: Dict[int, \"Tensor\"]\n",
    "    \"\"\"\n",
    "    Map from positional argument index to the Tensor at that position, in order to be able\n",
    "    to pass gradients back along the computational graph.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_back_func_lookup` passed!\n"
     ]
    }
   ],
   "source": [
    "class BackwardFuncLookup:\n",
    "    def __init__(self) -> None:\n",
    "        self.lookup = {}\n",
    "\n",
    "    def add_back_func(self, forward_fn: Callable, arg_position: int, back_fn: Callable) -> None:\n",
    "        self.lookup[(forward_fn, arg_position)] = back_fn\n",
    "\n",
    "    def get_back_func(self, forward_fn: Callable, arg_position: int) -> Callable:\n",
    "        return self.lookup[(forward_fn, arg_position)]\n",
    "\n",
    "\n",
    "utils.test_back_func_lookup(BackwardFuncLookup)\n",
    "\n",
    "BACK_FUNCS = BackwardFuncLookup()\n",
    "BACK_FUNCS.add_back_func(np.log, 0, log_back)\n",
    "BACK_FUNCS.add_back_func(np.multiply, 0, multiply_back0)\n",
    "BACK_FUNCS.add_back_func(np.multiply, 1, multiply_back1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    '''\n",
    "    A drop-in replacement for torch.Tensor supporting a subset of features.\n",
    "    '''\n",
    "\n",
    "    array: Arr\n",
    "    \"The underlying array. Can be shared between multiple Tensors.\"\n",
    "    requires_grad: bool\n",
    "    \"If True, calling functions or methods on this tensor will track relevant data for backprop.\"\n",
    "    grad: Optional[\"Tensor\"]\n",
    "    \"Backpropagation will accumulate gradients into this field.\"\n",
    "    recipe: Optional[Recipe]\n",
    "    \"Extra information necessary to run backpropagation.\"\n",
    "\n",
    "    def __init__(self, array: Union[Arr, list], requires_grad=False):\n",
    "        self.array = array if isinstance(array, Arr) else np.array(array)\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "        self.recipe = None\n",
    "        \"If not None, this tensor's array was created via recipe.func(*recipe.args, **recipe.kwargs).\"\n",
    "\n",
    "    def __neg__(self) -> \"Tensor\":\n",
    "        return negative(self)\n",
    "\n",
    "    def __add__(self, other) -> \"Tensor\":\n",
    "        return add(self, other)\n",
    "\n",
    "    def __radd__(self, other) -> \"Tensor\":\n",
    "        return add(other, self)\n",
    "\n",
    "    def __sub__(self, other) -> \"Tensor\":\n",
    "        return subtract(self, other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return subtract(other, self)\n",
    "\n",
    "    def __mul__(self, other) -> \"Tensor\":\n",
    "        return multiply(self, other)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return multiply(other, self)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return true_divide(self, other)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return true_divide(self, other)\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        return matmul(self, other)\n",
    "\n",
    "    def __rmatmul__(self, other):\n",
    "        return matmul(other, self)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return eq(self, other)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({repr(self.array)}, requires_grad={self.requires_grad})\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self.array.ndim == 0:\n",
    "            raise TypeError\n",
    "        return self.array.shape[0]\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return id(self)\n",
    "\n",
    "    def __getitem__(self, index) -> \"Tensor\":\n",
    "        return getitem(self, index)\n",
    "\n",
    "    def add_(self, other: \"Tensor\", alpha: float = 1.0) -> \"Tensor\":\n",
    "        add_(self, other, alpha=alpha)\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def T(self) -> \"Tensor\":\n",
    "        return permute(self)\n",
    "\n",
    "    def item(self):\n",
    "        return self.array.item()\n",
    "\n",
    "    def sum(self, dim=None, keepdim=False):\n",
    "        return sum(self, dim=dim, keepdim=keepdim)\n",
    "\n",
    "    def log(self):\n",
    "        return log(self)\n",
    "\n",
    "    def exp(self):\n",
    "        return exp(self)\n",
    "\n",
    "    def reshape(self, new_shape):\n",
    "        return reshape(self, new_shape)\n",
    "\n",
    "    def expand(self, new_shape):\n",
    "        return expand(self, new_shape)\n",
    "\n",
    "    def permute(self, dims):\n",
    "        return permute(self, dims)\n",
    "\n",
    "    def maximum(self, other):\n",
    "        return maximum(self, other)\n",
    "\n",
    "    def relu(self):\n",
    "        return relu(self)\n",
    "\n",
    "    def argmax(self, dim=None, keepdim=False):\n",
    "        return argmax(self, dim=dim, keepdim=keepdim)\n",
    "\n",
    "    def uniform_(self, low: float, high: float) -> \"Tensor\":\n",
    "        self.array[:] = np.random.uniform(low, high, self.array.shape)\n",
    "        return self\n",
    "\n",
    "    def backward(self, end_grad: Union[Arr, \"Tensor\", None] = None) -> None:\n",
    "        if isinstance(end_grad, Arr):\n",
    "            end_grad = Tensor(end_grad)\n",
    "        return backprop(self, end_grad)\n",
    "\n",
    "    def size(self, dim: Optional[int] = None):\n",
    "        if dim is None:\n",
    "            return self.shape\n",
    "        return self.shape[dim]\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.array.shape\n",
    "\n",
    "    @property\n",
    "    def ndim(self):\n",
    "        return self.array.ndim\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        '''Same as https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html'''\n",
    "        if self.requires_grad and self.recipe and self.recipe.parents:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __bool__(self):\n",
    "        if np.array(self.shape).prod() != 1:\n",
    "            raise RuntimeError(\"bool value of Tensor with more than one value is ambiguous\")\n",
    "        return bool(self.item())\n",
    "\n",
    "def empty(*shape: int) -> Tensor:\n",
    "    '''Like torch.empty.'''\n",
    "    return Tensor(np.empty(shape))\n",
    "\n",
    "def zeros(*shape: int) -> Tensor:\n",
    "    '''Like torch.zeros.'''\n",
    "    return Tensor(np.zeros(shape))\n",
    "\n",
    "def arange(start: int, end: int, step=1) -> Tensor:\n",
    "    '''Like torch.arange(start, end).'''\n",
    "    return Tensor(np.arange(start, end, step=step))\n",
    "\n",
    "def tensor(array: Arr, requires_grad=False) -> Tensor:\n",
    "    '''Like torch.tensor.'''\n",
    "    return Tensor(array, requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_tracking_enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_log` passed!\n",
      "All tests in `test_log_no_grad` passed!\n"
     ]
    }
   ],
   "source": [
    "def log_forward(x: Tensor) -> Tensor:\n",
    "    out_array = np.log(x.array)\n",
    "    out = Tensor(out_array)\n",
    "\n",
    "    if grad_tracking_enabled and (x.requires_grad or x.recipe is not None):\n",
    "        out.requires_grad = True\n",
    "        out.recipe = Recipe(func=np.log, args=(x.array,), kwargs={}, parents={0: x})\n",
    "\n",
    "    return out\n",
    "\n",
    "log = log_forward\n",
    "utils.test_log(Tensor, log_forward)\n",
    "utils.test_log_no_grad(Tensor, log_forward)\n",
    "a = Tensor([1], requires_grad=True)\n",
    "grad_tracking_enabled = False\n",
    "b = log_forward(a)\n",
    "grad_tracking_enabled = True\n",
    "assert not b.requires_grad, \"should not require grad if grad tracking globally disabled\"\n",
    "assert b.recipe is None, \"should not create recipe if grad tracking globally disabled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_multiply` passed!\n",
      "All tests in `test_multiply_no_grad` passed!\n",
      "All tests in `test_multiply_float` passed!\n"
     ]
    }
   ],
   "source": [
    "def multiply_forward(a: Union[Tensor, int], b: Union[Tensor, int]) -> Tensor:\n",
    "    assert isinstance(a, Tensor) or isinstance(b, Tensor)\n",
    "    \n",
    "    # Deal with cases where a, b are ints or Tensors, then calculate output\n",
    "    arg_a = a.array if isinstance(a, Tensor) else a\n",
    "    arg_b = b.array if isinstance(b, Tensor) else b\n",
    "    out_arr = np.multiply(arg_a, arg_b)\n",
    "\n",
    "    requires_grad = grad_tracking_enabled and any([\n",
    "        isinstance(x, Tensor) and (x.requires_grad or x.recipe is not None) for x in (a, b)\n",
    "    ])\n",
    "\n",
    "    out = Tensor(out_arr)\n",
    "    if requires_grad:\n",
    "        out.requires_grad = True\n",
    "        out.recipe = Recipe(\n",
    "            func=np.multiply,\n",
    "            args=(arg_a, arg_b),\n",
    "            kwargs={},\n",
    "            parents={idx: arr for idx, arr in enumerate([a, b]) if isinstance(arr, Tensor)}\n",
    "        )\n",
    "\n",
    "    return out\n",
    "\n",
    "multiply = multiply_forward\n",
    "utils.test_multiply(Tensor, multiply_forward)\n",
    "utils.test_multiply_no_grad(Tensor, multiply_forward)\n",
    "utils.test_multiply_float(Tensor, multiply_forward)\n",
    "a = Tensor([2], requires_grad=True)\n",
    "b = Tensor([3], requires_grad=True)\n",
    "grad_tracking_enabled = False\n",
    "b = multiply_forward(a, b)\n",
    "grad_tracking_enabled = True\n",
    "assert not b.requires_grad, \"should not require grad if grad tracking globally disabled\"\n",
    "assert b.recipe is None, \"should not create recipe if grad tracking globally disabled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_log` passed!\n",
      "All tests in `test_log_no_grad` passed!\n",
      "All tests in `test_multiply` passed!\n",
      "All tests in `test_multiply_no_grad` passed!\n",
      "All tests in `test_multiply_float` passed!\n",
      "All tests in `test_sum` passed!\n",
      "Got a nice exception as intended:\n",
      "log() takes from 1 to 2 positional arguments but 0 were given\n"
     ]
    }
   ],
   "source": [
    "def wrap_forward_fn(numpy_func: Callable, is_differentiable=True) -> Callable:\n",
    "    '''\n",
    "    numpy_func: function. It takes any number of positional arguments, some of which may be\n",
    "        NumPy arrays, and any number of keyword arguments which we aren't allowing to be NumPy\n",
    "        arrays at present. It returns a single NumPy array.\n",
    "    is_differentiable: if True, numpy_func is differentiable with respect to some input argument,\n",
    "        so we may need to track information in a Recipe. If False, we definitely don't need to\n",
    "        track information.\n",
    "\n",
    "    Return: function. It has the same signature as numpy_func, except wherever there was a\n",
    "        NumPy array, this has a Tensor instead.\n",
    "    '''\n",
    "\n",
    "    def tensor_func(*args: Any, **kwargs: Any) -> Tensor:\n",
    "        \n",
    "        arg_arrays = [(a.array if isinstance(a, Tensor) else a) for a in args]\n",
    "        out_arr = numpy_func(*arg_arrays, **kwargs)\n",
    "        \n",
    "        requires_grad = grad_tracking_enabled and is_differentiable and any([\n",
    "            (isinstance(a, Tensor) and (a.requires_grad or a.recipe is not None)) for a in args\n",
    "        ])\n",
    "        \n",
    "        out = Tensor(out_arr, requires_grad)\n",
    "        \n",
    "        if requires_grad:\n",
    "            parents = {idx: a for idx, a in enumerate(args) if isinstance(a, Tensor)}\n",
    "            out.recipe = Recipe(numpy_func, arg_arrays, kwargs, parents)\n",
    "            \n",
    "        return out\n",
    "\n",
    "    return tensor_func\n",
    "\n",
    "log = wrap_forward_fn(np.log)\n",
    "multiply = wrap_forward_fn(np.multiply)\n",
    "# need to be careful with sum, because kwargs have different names in torch and numpy\n",
    "def _sum(x: Arr, dim=None, keepdim=False) -> Arr:\n",
    "    return np.sum(x, axis=dim, keepdims=keepdim)\n",
    "sum = wrap_forward_fn(_sum)\n",
    "\n",
    "utils.test_log(Tensor, log)\n",
    "utils.test_log_no_grad(Tensor, log)\n",
    "utils.test_multiply(Tensor, multiply)\n",
    "utils.test_multiply_no_grad(Tensor, multiply)\n",
    "utils.test_multiply_float(Tensor, multiply)\n",
    "utils.test_sum(Tensor)\n",
    "try:\n",
    "    log(x=Tensor([100]))\n",
    "except Exception as e:\n",
    "    print(\"Got a nice exception as intended:\")\n",
    "    print(e)\n",
    "else:\n",
    "    assert False, \"Passing tensor by keyword should raise some informative exception.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_topological_sort_linked_list` passed!\n",
      "All tests in `test_topological_sort_branching` passed!\n",
      "All tests in `test_topological_sort_rejoining` passed!\n",
      "All tests in `test_topological_sort_cyclic` passed!\n"
     ]
    }
   ],
   "source": [
    "class Node:\n",
    "    def __init__(self, *children):\n",
    "        self.children = list(children)\n",
    "\n",
    "\n",
    "def topological_sort(node: Node) -> List[Any]:\n",
    "    '''\n",
    "    Return a list of node's descendants in reverse topological order, from future to past.\n",
    "\n",
    "    Should raise an error if the graph with `node` as root is not in fact acyclic.\n",
    "    '''\n",
    "    # Note, you can also add `perm`, which stores contents of `results` in a set - this is computationally faster\n",
    "\n",
    "    result = [] # stores the list of nodes to be returned (in reverse topological order)\n",
    "    temp = set() # keeps track of previously visited nodes (to detect cyclicity)\n",
    "\n",
    "    def visit(cur: Node):\n",
    "        \"\"\"\n",
    "        Recursive function which visits all the children of the current node\n",
    "        \"\"\"\n",
    "        if cur in result:\n",
    "            return\n",
    "        if cur in temp:\n",
    "            raise ValueError(\"Not a DAG!\")\n",
    "        temp.add(cur)\n",
    "\n",
    "        for next in cur.children:\n",
    "            visit(next)\n",
    "\n",
    "        temp.remove(cur)\n",
    "        result.append(cur)\n",
    "\n",
    "    visit(node)\n",
    "    return result\n",
    "\n",
    "utils.test_topological_sort_linked_list(topological_sort)\n",
    "utils.test_topological_sort_branching(topological_sort)\n",
    "utils.test_topological_sort_rejoining(topological_sort)\n",
    "utils.test_topological_sort_cyclic(topological_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_eq(a, b):\n",
    "    return id(a) == id(b)\n",
    "    # return (a.array == b.array).all() and (a.recipe == b.recipe)\n",
    "\n",
    "eq = tensor_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['g', 'f', 'e', 'c', 'd', 'b', 'a']\n"
     ]
    }
   ],
   "source": [
    "def sorted_computational_graph(node: Tensor) -> List[Tensor]:\n",
    "    '''\n",
    "    For a given tensor, return a list of Tensors that make up the nodes of the given Tensor's computational graph, in reverse topological order.\n",
    "    '''\n",
    "    # Note, you can also add `perm`, which stores contents of `results` in a set - this is computationally faster\n",
    "\n",
    "    result = [] # stores the list of nodes to be returned (in reverse topological order)\n",
    "    temp = set() # keeps track of previously visited nodes (to detect cyclicity)\n",
    "\n",
    "    def get_parents(node: Tensor):\n",
    "        if node.recipe is not None:\n",
    "            return list(node.recipe.parents.values())\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def visit(cur: Tensor):\n",
    "        \"\"\"\n",
    "        Recursive function which visits all the children of the current node\n",
    "        \"\"\"\n",
    "        if cur in result:\n",
    "            return\n",
    "        if cur in temp:\n",
    "            raise ValueError(\"Not a DAG!\")\n",
    "        temp.add(cur)\n",
    "\n",
    "        for next in get_parents(cur):\n",
    "            visit(next)\n",
    "\n",
    "        temp.remove(cur)\n",
    "        result.append(cur)\n",
    "\n",
    "    visit(node)\n",
    "    return result[::-1]\n",
    "\n",
    "a = Tensor([1], requires_grad=True)\n",
    "b = Tensor([2], requires_grad=True)\n",
    "c = Tensor([3], requires_grad=True)\n",
    "d = a * b\n",
    "e = c.log()\n",
    "f = d * e\n",
    "g = f.log()\n",
    "name_lookup = {a: \"a\", b: \"b\", c: \"c\", d: \"d\", e: \"e\", f: \"f\", g: \"g\"}\n",
    "\n",
    "print([name_lookup[t] for t in sorted_computational_graph(g)])\n",
    "# Should get something in reverse alphabetical order (or close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_backprop` passed!\n",
      "All tests in `test_backprop_branching` passed!\n",
      "All tests in `test_backprop_requires_grad_false` passed!\n",
      "All tests in `test_backprop_float_arg` passed!\n"
     ]
    }
   ],
   "source": [
    "def backprop(end_node: Tensor, end_grad: Optional[Tensor] = None):\n",
    "# def backprop(end_node: Tensor, end_grad: Optional[Tensor] = None) -> None:\n",
    "    \"\"\"Accumulates gradients in the grad field of each leaf node.\n",
    "\n",
    "    tensor.backward() is equivalent to backprop(tensor).\n",
    "\n",
    "    end_node: \n",
    "        The rightmost node in the computation graph. \n",
    "        If it contains more than one element, end_grad must be provided.\n",
    "    end_grad: \n",
    "        A tensor of the same shape as end_node. \n",
    "        Set to 1 if not specified and end_node has only one element.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get value of end_grad_arr\n",
    "    end_grad_arr = np.ones_like(end_node.array) if end_grad is None else end_grad.array\n",
    "    \n",
    "    # Create dict to store gradients\n",
    "    grads: Dict[Tensor, Arr] = {end_node: end_grad_arr}\n",
    "\n",
    "    # Iterate through the computational graph, using your sorting function\n",
    "    for node in sorted_computational_graph(end_node):\n",
    "        \n",
    "        # Get the outgradient (recall we need it in our backward functions)\n",
    "        outgrad = grads.pop(node)\n",
    "        # We only store the gradients if this node is a leaf (see the is_leaf property of Tensor)\n",
    "        if node.is_leaf and node.requires_grad:\n",
    "            # Add the gradient to this node's grad (need to deal with special case grad=None)\n",
    "            if node.grad is None:\n",
    "                node.grad = Tensor(outgrad)\n",
    "            else:\n",
    "                node.grad.array += outgrad\n",
    "                \n",
    "        # If node has no recipe, then it has no parents, i.e. the backtracking through computational\n",
    "        # graph ends here\n",
    "        if node.recipe is None:\n",
    "            continue\n",
    "            \n",
    "        # If node has a recipe, then we iterate through parents (which is a dict of {arg_posn: tensor})\n",
    "        for argnum, parent in node.recipe.parents.items():\n",
    "            \n",
    "            # Get the backward function corresponding to the function that created this node,\n",
    "            # and the arg posn of this particular parent within that function \n",
    "            back_fn = BACK_FUNCS.get_back_func(node.recipe.func, argnum)\n",
    "            \n",
    "            # Use this backward function to calculate the gradient\n",
    "            in_grad = back_fn(outgrad, node.array, *node.recipe.args, **node.recipe.kwargs)\n",
    "            \n",
    "            # Add the gradient to this node in the dictionary `grads`\n",
    "            # Note that we only change the grad of the node itself in the code block above\n",
    "            if grads.get(parent) is None:\n",
    "                grads[parent] = in_grad\n",
    "            else:\n",
    "                grads[parent] += in_grad\n",
    "\n",
    "\n",
    "utils.test_backprop(Tensor)\n",
    "utils.test_backprop_branching(Tensor)\n",
    "utils.test_backprop_requires_grad_false(Tensor)\n",
    "utils.test_backprop_float_arg(Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84205e65a427e19ff91dd8a92d5a5f2cf0946f21e31957c24c166022f0f50b37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
