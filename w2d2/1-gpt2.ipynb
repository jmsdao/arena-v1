{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Add this file to the Colab workspace\n",
        "# https://github.com/jmsdao/arena-v1/blob/jmsdao/w2d2/utils.py"
      ],
      "metadata": {
        "id": "EPUCXRriREan"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers fancy-einsum einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gBA7tOuYGxE",
        "outputId": "c1f1cc45-2990-40b1-9570-b37e78f8eb72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: fancy-einsum in /usr/local/lib/python3.7/dist-packages (0.0.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "\n",
        "from fancy_einsum import einsum\n",
        "import math\n",
        "import utils"
      ],
      "metadata": {
        "id": "vancQKXA3S6B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multihead_masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, num_heads: int):\n",
        "    '''\n",
        "    Implements multihead masked attention on the matrices Q, K and V.\n",
        "    Q: shape (batch, seq, nheads*headsize)\n",
        "    K: shape (batch, seq, nheads*headsize)\n",
        "    V: shape (batch, seq, nheads*headsize)\n",
        "    '''\n",
        "    batch = Q.shape[0]\n",
        "    seq_len = Q.shape[1]\n",
        "    headsize = Q.shape[2] // num_heads\n",
        "\n",
        "    Q = Q.reshape(batch, seq_len, num_heads, headsize)\n",
        "    K = K.reshape(batch, seq_len, num_heads, headsize)\n",
        "    V = V.reshape(batch, seq_len, num_heads, headsize)\n",
        "\n",
        "    scale = t.sqrt(t.tensor(K.shape[-1]).type(t.float32))\n",
        "    raw_attention_filter = einsum('b sl_Q nh hs, b sl_K nh hs -> b nh sl_Q sl_K', Q, K)\n",
        "    mask_filter = t.triu(t.full_like(raw_attention_filter, -t.inf), 1)\n",
        "    masked_attention_filter = t.softmax((raw_attention_filter + mask_filter) / scale, dim=-1)\n",
        "    attention_values = einsum('b nh sl_Q sl_K, b sl_K nh hs -> b sl_Q nh hs', masked_attention_filter, V)\n",
        "    return attention_values.reshape(batch, seq_len, num_heads * headsize)\n",
        "\n",
        "\n",
        "class MultiheadMaskedAttention(nn.Module):\n",
        "    W_QKV: nn.Linear\n",
        "    W_O: nn.Linear\n",
        "\n",
        "    def __init__(self, hidden_size: int, num_heads: int):\n",
        "        assert hidden_size % num_heads == 0, \"num_heads should be divisible by hidden_size\"\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.W_QKV = nn.Linear(hidden_size, 3 * hidden_size)\n",
        "        self.W_O = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        '''\n",
        "        x: shape (batch, seq, hidden_size)\n",
        "        Return: shape (batch, seq, hidden_size)\n",
        "        '''\n",
        "        headsize = self.hidden_size // self.num_heads\n",
        "\n",
        "        QKV = self.W_QKV(x)        \n",
        "        Q = QKV[..., :self.hidden_size]\n",
        "        K = QKV[..., self.hidden_size:2*self.hidden_size]\n",
        "        V = QKV[..., 2*self.hidden_size:3*self.hidden_size]\n",
        "        attention_values = multihead_masked_attention(Q, K, V, self.num_heads)\n",
        "        return self.W_O(attention_values)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TransformerConfig:\n",
        "    '''Constants used throughout your decoder-only transformer model.'''\n",
        "    num_layers: int\n",
        "    num_heads: int\n",
        "    vocab_size: int\n",
        "    hidden_size: int\n",
        "    max_seq_len: int\n",
        "    dropout: float = 0.1\n",
        "    layer_norm_epsilon: float = 1e-05\n",
        "    print_param_count: bool = False"
      ],
      "metadata": {
        "id": "v57gKIP68CzF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewGELUActivation(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
        "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, input: t.Tensor) -> t.Tensor:\n",
        "        return 0.5 * input * (1.0 + t.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * t.pow(input, 3.0))))\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size, 4*config.hidden_size),\n",
        "            # nn.GELU(),\n",
        "            NewGELUActivation(),\n",
        "            nn.Linear(4*config.hidden_size, config.hidden_size),\n",
        "            nn.Dropout(config.dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class GPTDecoderBlock(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
        "        self.att = MultiheadMaskedAttention(config.hidden_size, config.num_heads)\n",
        "        self.ln2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        x = x + self.att(self.ln1(x))\n",
        "        return x + self.mlp(self.ln2(x))"
      ],
      "metadata": {
        "id": "hXgKfXUG-M3g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyGPT(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.positional_encoding = nn.Embedding(config.max_seq_len, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.decoder_blocks = nn.Sequential(\n",
        "            *[GPTDecoderBlock(config) for _ in range(config.num_layers)]\n",
        "        )\n",
        "        self.final_ln = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        '''\n",
        "        x is a tensor of token ids\n",
        "        x: shape (batch, seq_len)\n",
        "        Return: shape (batch, seq_len, vocab_size)\n",
        "        '''\n",
        "        if x.dim() == 1:  # add batch dimension if missing\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        pos = t.arange(x.shape[1], device=x.device)\n",
        "        x = self.token_embedding(x) + self.positional_encoding(pos)\n",
        "        x = self.dropout(x)\n",
        "        x = self.decoder_blocks(x)\n",
        "        x = self.final_ln(x)\n",
        "        x = einsum(\"batch seq hidden, vocab hidden -> batch seq vocab\", x, self.token_embedding.weight)\n",
        "        return x"
      ],
      "metadata": {
        "id": "NEXshLCB7MuT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_weights(my_gpt: MyGPT, gpt) -> MyGPT:\n",
        "    '''Copy over the weights of `gpt` to your gpt implementation.'''\n",
        "\n",
        "    # Here we use named params not state dict, because gpt doesn't have any buffers we care about\n",
        "    # (I think all its buffers are attention masks)\n",
        "    my_gpt_dict = dict(my_gpt.named_parameters())\n",
        "    gpt_dict = dict(gpt.named_parameters())\n",
        "    \n",
        "    # Check the number of params/buffers is correct\n",
        "    assert len(my_gpt_dict) == len(gpt_dict), \"Number of layers is wrong. Have you done the prev step correctly?\"\n",
        "    \n",
        "    # Initialise an empty dictionary to store the correct key-value pairs\n",
        "    state_dict = {}\n",
        "    \n",
        "    for (my_param_name, my_param), (name, param) in zip(my_gpt_dict.items(), gpt_dict.items()):\n",
        "        # Sometimes params are transposed\n",
        "        if len(my_param.shape) == 2 and my_param.shape == param.T.shape:\n",
        "            state_dict[my_param_name] = param.T\n",
        "            # print(f\"Copied params.T: {name} -> {my_param_name}\")\n",
        "        elif my_param.shape == param.shape:\n",
        "            state_dict[my_param_name] = param\n",
        "            # print(f\"Copied params:   {name} -> {my_param_name}\")\n",
        "        else:\n",
        "            raise Exception(f\"Parameter shapes don't match: {my_param.shape} vs {param.shape}\")\n",
        "\n",
        "    if set(state_dict.keys()) != set(my_gpt.state_dict().keys()):\n",
        "        raise Exception(\"State dicts don't match.\")\n",
        "    \n",
        "    my_gpt.load_state_dict(state_dict)\n",
        "    \n",
        "    return my_gpt"
      ],
      "metadata": {
        "id": "g3IeMe71BJdz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = TransformerConfig(\n",
        "    num_layers = 12,\n",
        "    num_heads = 12,\n",
        "    vocab_size = 50257,\n",
        "    hidden_size = 768,\n",
        "    max_seq_len = 1024,\n",
        "    dropout = 0.1,\n",
        "    layer_norm_epsilon = 1e-05,\n",
        "    print_param_count = False\n",
        ")"
      ],
      "metadata": {
        "id": "HEXiXXvPW1-d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mygpt = MyGPT(config).eval()\n",
        "gpt2 = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\").eval()"
      ],
      "metadata": {
        "id": "fKBWjcESBRVp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utils.print_param_count(mygpt, gpt2)"
      ],
      "metadata": {
        "id": "1ySozmf-Wv2o"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mygpt = copy_weights(mygpt, gpt2)"
      ],
      "metadata": {
        "id": "GW3SOmc-WwzV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "alvFCLv89kq2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text: str) -> t.Tensor:\n",
        "    \"\"\"Return a Tensor of shape (batch=1, seq).\"\"\"\n",
        "    return tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "def get_next_tokens(model, tokenizer, prompt):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    input_ids = encode(prompt).to(device)\n",
        "    with t.inference_mode():\n",
        "        output = model(input_ids)\n",
        "        logits = output[0, -1] if isinstance(output, t.Tensor) else output.logits[0, -1]\n",
        "    topk = t.topk(logits, k=10).indices\n",
        "    next_tokens = tokenizer.batch_decode(topk.reshape(-1, 1))\n",
        "    return next_tokens"
      ],
      "metadata": {
        "id": "8uEoEIibChKb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Former President of the United States of America, George\"\n",
        "get_next_tokens(gpt2, tokenizer, prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adnSv0722usW",
        "outputId": "fd8ef4bf-15ec-4fad-f0fb-0b6933f242d7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' W',\n",
              " ' H',\n",
              " ' Bush',\n",
              " ' Washington',\n",
              " ' HW',\n",
              " ' Herbert',\n",
              " ' Pat',\n",
              " ' S',\n",
              " ' Soros',\n",
              " ' Wallace']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Former President of the United States of America, George\"\n",
        "get_next_tokens(mygpt, tokenizer, prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsIjBKh12uvx",
        "outputId": "de6a7d83-2ee1-459b-e776-fbb9798a849e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' W',\n",
              " ' H',\n",
              " ' Bush',\n",
              " ' Washington',\n",
              " ' HW',\n",
              " ' Herbert',\n",
              " ' Pat',\n",
              " ' S',\n",
              " ' Soros',\n",
              " ' Wallace']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "utils.test_load_pretrained_weights(mygpt, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okixGtnC2vEb",
        "outputId": "67c67ebe-4b71-4424-eff9-cec55a40c8f2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:  Former President of the United States of America, George\n",
            "Your model's top 10 predictions:  [' W', ' H', ' Bush', ' Washington', ' HW', ' Herbert', ' Pat', ' S', ' Soros', ' Wallace']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0TTrEs78XLCB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}