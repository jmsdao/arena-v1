{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "from fancy_einsum import einsum\n",
    "from typing import Union, Tuple, Optional\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IntOrPair = Union[int, Tuple[int, int]]\n",
    "Pair = Tuple[int, int]\n",
    "\n",
    "def force_pair(v: IntOrPair) -> Pair:\n",
    "    '''Convert v to a pair of int, if it isn't already.'''\n",
    "    if isinstance(v, tuple):\n",
    "        if len(v) != 2:\n",
    "            raise ValueError(v)\n",
    "        return (int(v[0]), int(v[1]))\n",
    "    elif isinstance(v, int):\n",
    "        return (v, v)\n",
    "    raise ValueError(v)\n",
    "\n",
    "# Examples of how this function can be used:\n",
    "#       force_pair((1, 2))     ->  (1, 2)\n",
    "#       force_pair(2)          ->  (2, 2)\n",
    "#       force_pair((1, 2, 3))  ->  ValueError\n",
    "\n",
    "\n",
    "def pad2d(x: t.Tensor, left: int, right: int, top: int, bottom: int, pad_value: float) -> t.Tensor:\n",
    "    '''Return a new tensor with padding applied to the edges.\n",
    "\n",
    "    x: shape (batch, in_channels, height, width), dtype float32\n",
    "\n",
    "    Return: shape (batch, in_channels, top + height + bottom, left + width + right)\n",
    "    '''\n",
    "    b, c, h, w = x.shape\n",
    "    x_padded = x.new_full((b, c, top + h + bottom, left + w + right), pad_value)\n",
    "    x_padded[:, :, top:top+h, left:left+w] = x\n",
    "    return x_padded\n",
    "\n",
    "\n",
    "def maxpool2d(x: t.Tensor, kernel_size: IntOrPair, stride: Optional[IntOrPair] = None, padding: IntOrPair = 0) -> t.Tensor:\n",
    "    '''Like PyTorch's maxpool2d.\n",
    "\n",
    "    x: shape (batch, channels, height, width)\n",
    "    stride: if None, should be equal to the kernel size\n",
    "\n",
    "    Return: (batch, channels, out_height, output_width)\n",
    "    '''\n",
    "    if stride is None:\n",
    "        stride = kernel_size\n",
    "\n",
    "    kh, kw = force_pair(kernel_size)\n",
    "    padding_h, padding_w = force_pair(padding)\n",
    "    stride_h, stride_w = force_pair(stride)\n",
    "    \n",
    "    x = pad2d(x, padding_w, padding_w, padding_h, padding_h, -t.inf)\n",
    "    \n",
    "    b, ic, ih, iw = x.shape         # batch, in_channels, input_height, input_width\n",
    "    oh = (ih - kh) // stride_h + 1  # output_height\n",
    "    ow = (iw - kw) // stride_w + 1  # output_width\n",
    "\n",
    "    bs, ics, ihs, iws = x.stride()  # batch_stride, input_channel_stride, input_height_stride, input_width_stride\n",
    "    x_strided = x.as_strided(\n",
    "        size=(b, ic, oh, ow, kh, kw),\n",
    "        stride=(bs, ics, ihs * stride_h, iws * stride_w, ihs, iws)\n",
    "    )\n",
    "\n",
    "    return x_strided.amax((-1, -2))\n",
    "\n",
    "\n",
    "def conv2d(x, weights, stride: IntOrPair = 1, padding: IntOrPair = 0) -> t.Tensor:\n",
    "    '''Like torch's conv2d using bias=False\n",
    "\n",
    "    x: shape (batch, in_channels, height, width)\n",
    "    weights: shape (out_channels, in_channels, kernel_height, kernel_width)\n",
    "\n",
    "    Returns: shape (batch, out_channels, output_height, output_width)\n",
    "    '''\n",
    "    padding_h, padding_w = force_pair(padding)\n",
    "    stride_h, stride_w = force_pair(stride)\n",
    "    \n",
    "    x = pad2d(x, padding_w, padding_w, padding_h, padding_h, 0)\n",
    "    \n",
    "    b, ic, ih, iw = x.shape         # batch, in_channels, input_height, input_width\n",
    "    oc, ic, kh, kw = weights.shape  # out_channels, in_channels, kernel_height, kernel_width\n",
    "    oh = (ih - kh) // stride_h + 1  # output_height\n",
    "    ow = (iw - kw) // stride_w + 1  # output_width\n",
    "\n",
    "    bs, ics, ihs, iws = x.stride()  # batch_stride, input_channel_stride, input_height_stride, input_width_stride\n",
    "    x_strided = x.as_strided(\n",
    "        size=(b, ic, oh, ow, kh, kw),\n",
    "        stride=(bs, ics, ihs * stride_h, iws * stride_w, ihs, iws)\n",
    "    )\n",
    "\n",
    "    return einsum('b ic oh ow kh kw, oc ic kh kw -> b oc oh ow', x_strided, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually verify that this is an informative repr: MaxPool2d(kernel_size=3, stride=2, padding=1)\n"
     ]
    }
   ],
   "source": [
    "class MaxPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size: IntOrPair, stride: Optional[IntOrPair] = None, padding: IntOrPair = 0):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Call the functional version of maxpool2d.'''\n",
    "        return maxpool2d(x, self.kernel_size, self.stride, self.padding)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        '''Add additional information to the string representation of this class.'''\n",
    "        return f'kernel_size={self.kernel_size}, stride={self.stride}, padding={self.padding}'\n",
    "\n",
    "utils.test_maxpool2d_module(MaxPool2d)\n",
    "m = MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "print(f\"Manually verify that this is an informative repr: {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(nn.Module):\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        return t.max(x, t.tensor(0.0))\n",
    "\n",
    "utils.test_relu(ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repr: Flatten(start_dim=1, end_dim=-1)\n"
     ]
    }
   ],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self, start_dim: int = 1, end_dim: int = -1) -> None:\n",
    "        super().__init__()\n",
    "        self.start_dim = start_dim\n",
    "        self.end_dim = end_dim\n",
    "\n",
    "    def forward(self, input: t.Tensor) -> t.Tensor:\n",
    "        '''Flatten out dimensions from start_dim to end_dim, inclusive of both.\n",
    "        '''\n",
    "        start_dim = self.start_dim % input.dim()\n",
    "        end_dim = self.end_dim % input.dim()\n",
    "\n",
    "        dims = [f'd{i}' for i in range(input.dim())]\n",
    "        ein_left = ' '.join(dims)\n",
    "\n",
    "        dims[start_dim] = '(' + dims[start_dim]\n",
    "        dims[end_dim] = dims[end_dim] + ')'\n",
    "        ein_right = ' '.join(dims)\n",
    "\n",
    "        return rearrange(input, ein_left + ' -> ' + ein_right)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'start_dim={self.start_dim}, end_dim={self.end_dim}'\n",
    "\n",
    "utils.test_flatten(Flatten)\n",
    "m = Flatten(start_dim=1, end_dim=-1)\n",
    "print(f\"Repr: {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repr: Linear(in_features=5, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias=True):\n",
    "        '''A simple linear (technically, affine) transformation.\n",
    "\n",
    "        The fields should be named `weight` and `bias` for compatibility with PyTorch.\n",
    "        If `bias` is False, set `self.bias` to None.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        weight = 2 *(t.rand((in_features, out_features)) - 0.5) / t.sqrt(t.tensor(in_features))\n",
    "        self.weight = nn.Parameter(weight.T)  # transposing to pass asserts in test\n",
    "        \n",
    "        if bias:\n",
    "            bias = 2 *(t.rand(out_features) - 0.5) / t.sqrt(t.tensor(in_features))\n",
    "            self.bias = nn.Parameter(bias)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (*, in_features)\n",
    "        Return: shape (*, out_features)\n",
    "        '''\n",
    "        output = t.matmul(x, self.weight.T)\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"in_features={self.in_features}, out_features={self.out_features}, bias={True if self.bias is not None else False}\"\n",
    "\n",
    "utils.test_linear_forward(Linear)\n",
    "utils.test_linear_parameters(Linear)\n",
    "utils.test_linear_no_bias(Linear)\n",
    "\n",
    "m = Linear(5, 3)\n",
    "print(f\"Repr: {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repr: Conv2d(in_channels=2, out_channels=3, kernel_size=2, stride=1, padding=0)\n"
     ]
    }
   ],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, kernel_size: IntOrPair, stride: IntOrPair = 1, padding: IntOrPair = 0\n",
    "    ):\n",
    "        '''\n",
    "        Same as torch.nn.Conv2d with bias=False.\n",
    "\n",
    "        Name your weight field `self.weight` for compatibility with the PyTorch version.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        kernel_height, kernel_width = force_pair(kernel_size)\n",
    "        xavier = t.sqrt(t.tensor(in_channels * kernel_height * kernel_width))\n",
    "        weight = 2 *(t.rand(out_channels, in_channels, kernel_height, kernel_width) - 0.5) / xavier\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Apply the functional conv2d you wrote earlier.'''\n",
    "        return conv2d(x, self.weight, self.stride, self.padding)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        keys = [\"in_channels\", \"out_channels\", \"kernel_size\", \"stride\", \"padding\"]\n",
    "        return \", \".join([f\"{key}={getattr(self, key)}\" for key in keys])\n",
    "\n",
    "utils.test_conv2d_module(Conv2d)\n",
    "\n",
    "m = Conv2d(2, 3, 2)\n",
    "print(f\"Repr: {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84205e65a427e19ff91dd8a92d5a5f2cf0946f21e31957c24c166022f0f50b37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
